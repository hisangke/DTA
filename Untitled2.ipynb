{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'diving': 0, 'tennis': 9, 'walk': 10, 'biking': 1, 'golf': 2, 'juggle': 3, 'jumping': 4, 'riding': 5, 'shooting': 6, 'spiking': 7, 'swing': 8}\n",
      "successfully,len(train_list) 954\n",
      "successfully,len(test_list) 645\n",
      "WARNING:tensorflow:From C:\\soft\\install\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-1-0709451c34d7>:149: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-1-0709451c34d7>:150: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
      "WARNING:tensorflow:From <ipython-input-1-0709451c34d7>:152: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From <ipython-input-1-0709451c34d7>:156: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From <ipython-input-1-0709451c34d7>:187: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-1-0709451c34d7>:200: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Tensor(\"add_3:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\soft\\install\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import glob\n",
    "import sys\n",
    "import h5py \n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from conv_cell import ConvLSTMCell\n",
    "from fc_attention import fc_attention_sum,fc_attention\n",
    "from conv_attention import conv_attention_sum,conv_attention\n",
    "\n",
    "########################################################################\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "conv_img_features = \"/your_dir/ucf11_features/image_pool5/\"\n",
    "fc_img_features = \"/your_dir/ucf11_features/image_fc6/\"\n",
    "\n",
    "\n",
    "train_list = \"C:\\\\Users\\\\user\\\\Desktop\\\\git\\\\DTA\\\\ucf11TrainTestlist\\\\ucf11_train.txt\"\n",
    "test_list = \"C:\\\\Users\\\\user\\\\Desktop\\\\git\\\\DTA\\\\ucf11TrainTestlist\\\\ucf11_test.txt\"\n",
    "classInd = \"C:\\\\Users\\\\user\\\\Desktop\\\\git\\\\DTA\\\\ucf11TrainTestlist\\\\classInd.txt\"\n",
    "\n",
    "batch_size = 6\n",
    "n_inputs = 4096   \n",
    "n_steps = 40    \n",
    "n_hidden_units = filters = 1024\n",
    "fc_attention_size = 50\n",
    "n_classes = 11\n",
    "n_layers = 2\n",
    "scale = 1\n",
    "n_num =10\n",
    "\n",
    "timesteps = num_frames = 40\n",
    "shape_1 = [7, 7]\n",
    "shape_2 = [5, 10]\n",
    "kernel = [3, 3]\n",
    "channels = 512\n",
    "attention_kernel = [1,1]\n",
    "basic_lr = 0.0001\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "g = open(classInd,\"r\")\n",
    "labels = sorted(g.readlines())\n",
    "nums = []\n",
    "names = []\n",
    "for label in labels:\n",
    "    a = label.split(\" \")\n",
    "    nums.append(int(a[0])-1)\n",
    "    names.append(a[1][:-1])\n",
    "label_dict = dict(zip(names,nums))\n",
    "print(label_dict)\n",
    "\n",
    "\n",
    "train_lines = []\n",
    "f = open(train_list,\"r\")  \n",
    "lines_ = f.readlines()\n",
    "for i in range(len(lines_)//scale):\n",
    "  train_lines.append(lines_[i*scale])\n",
    "len_train = len(train_lines)\n",
    "print(\"successfully,len(train_list)\",len_train)\n",
    "random.shuffle(train_lines)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "h = open(test_list,\"r\")  \n",
    "test_lines_ = sorted(h.readlines())\n",
    "test_lines = [] \n",
    "for i in range(len(test_lines_)//scale):\n",
    "  test_lines.append(test_lines_[i*scale])\n",
    "len_test = len(test_lines)\n",
    "print(\"successfully,len(test_list)\",len_test)\n",
    "\n",
    "\n",
    "\n",
    "test_labels = []\n",
    "ground_labels = []\n",
    "for test_video in test_lines:\n",
    "    video_class = str(test_video.split(\" \")).split(\"_\")[1]\n",
    "    ground_label = label_dict[video_class]\n",
    "    test_labels.append(ground_label)\n",
    "\n",
    "#print(test_labels)\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "\n",
    "\n",
    "def fc_feature(video_name):\n",
    "\n",
    "    g = h5py.File(fc_img_features+video_name)\n",
    "    img_features = g['video_feature']\n",
    "    img_features = img_features[:]\n",
    "    g.close()\n",
    "    return img_features\n",
    "\n",
    "\n",
    "def conv_feature(video_name):\n",
    "    g = h5py.File(conv_img_features+video_name)\n",
    "    img_features = g['video_feature']\n",
    "    img_features = img_features[:]\n",
    "    g.close()\n",
    "    return img_features\n",
    "\n",
    "\n",
    "def accuracy(a,b):\n",
    "    c = np.equal(a,b).astype(float)\n",
    "    acc = sum(c)/len(c)\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def compute_score_loss(batch_fc_img,batch_conv_img,batch_labels):\n",
    "    global fc_pred\n",
    "    global conv_pred\n",
    "    global loss\n",
    "    fc_score,conv_score,video_loss =  sess.run([fc_pred,conv_pred,loss], feed_dict={\n",
    "            fc_img : batch_fc_img,\n",
    "            conv_img: batch_conv_img,\n",
    "            ys : batch_labels\n",
    "            })\n",
    "    return fc_score,conv_score,video_loss\n",
    "#################################################################################\n",
    "fc_img = tf.placeholder(tf.float32, [None, 40, 4096]) \n",
    "\n",
    "conv_img = tf.placeholder(tf.float32, [None, timesteps] + shape_1 + [channels]) \n",
    "\n",
    "\n",
    "\n",
    "ys = tf.placeholder(tf.float32, [None, n_classes])\n",
    "Lr = tf.placeholder(tf.float32) \n",
    "\n",
    "\n",
    "def FC_LSTM(X_spa,attention):\n",
    "    weights_img = tf.Variable(tf.random_normal([n_inputs, n_hidden_units]))\n",
    "    biases_img = tf.Variable(tf.constant(0.1, shape=[n_hidden_units, ]))\n",
    "\n",
    "\n",
    "    X_spa = tf.reshape(X_spa, [n_num*batch_size*n_steps, n_inputs])\n",
    "    X_spa = tf.matmul(X_spa, weights_img) + biases_img\n",
    "    X_spa = tf.reshape(X_spa, [-1, n_steps, n_hidden_units])\n",
    "\n",
    "\n",
    "    cell_spa = tf.contrib.rnn.BasicLSTMCell(n_hidden_units)\n",
    "    mlstm_cell_spa = tf.contrib.rnn.MultiRNNCell([cell_spa for _ in range(n_layers)], state_is_tuple=True)     \n",
    "    init_state_spa = mlstm_cell_spa.zero_state(n_num*batch_size, dtype=tf.float32)\n",
    "    outputs_spa, final_state_spa = tf.nn.dynamic_rnn(mlstm_cell_spa, X_spa, initial_state=init_state_spa, time_major=False)\n",
    "    \n",
    "    attention_output_spa = fc_attention_sum(outputs_spa, fc_attention_size)\n",
    "    if attention == True:\n",
    "        attention_output_spa = tf.layers.batch_normalization(attention_output_spa)\n",
    "        return attention_output_spa\n",
    "    else:\n",
    "        outputs_spa = tf.layers.batch_normalization(outputs_spa)\n",
    "        outputs_spa = tf.reduce_sum(outputs_spa,axis = 1)\n",
    "        return outputs_spa\n",
    "\n",
    "def CONV_LSTM(conv_img,shape,attention):\n",
    "    img_cell = ConvLSTMCell(shape, filters, kernel)\n",
    "    img_outputs, img_state = tf.nn.dynamic_rnn(img_cell, conv_img, dtype=conv_img.dtype, time_major=True)\n",
    "    if attention == True: \n",
    "        img_attention_output = conv_attention_sum(img_outputs, attention_kernel)\n",
    "        img_attention_output = tf.layers.batch_normalization(img_attention_output)\n",
    "        return img_attention_output\n",
    "    else:\n",
    "        img_outputs = tf.layers.batch_normalization(img_outputs)\n",
    "        img_outputs = tf.reduce_sum(img_outputs,axis = 1)\n",
    "        return img_outputs  \n",
    "\n",
    "def FC_layer(inputs):\n",
    "    weights11 =  tf.get_variable(\"xiange\",[n_hidden_units, n_classes],initializer=tf.truncated_normal_initializer())\n",
    "    biases11 = tf.get_variable(\"ke\",[n_classes],initializer=tf.truncated_normal_initializer())\n",
    "    result = tf.nn.dropout((tf.matmul(inputs, weights11) + biases11), 0.5)\n",
    "    return result\n",
    "\n",
    "#fc_img = tf.placeholder(tf.float32, [None, 40, 4096]) \n",
    "#conv_img = tf.placeholder(tf.float32, [None, timesteps] + shape_1 + [channels]) \n",
    "\n",
    "fc_img_out = FC_LSTM(fc_img, True)\n",
    "conv_img_out = CONV_LSTM(conv_img, shape_1,True)\n",
    "\n",
    "fc_img_drop = tf.nn.dropout(fc_img_out, 0.5)\n",
    "conv_img_drop = tf.nn.dropout(conv_img_out, 0.5)\n",
    "conv_img_drop = tf.nn.max_pool(conv_img_drop,[1,7,7,1],[1,1,1,1],padding='VALID')\n",
    "conv_img_drop = tf.reshape(conv_img_drop,[-1,filters])\n",
    "\n",
    "with tf.variable_scope(\"FC1\",reuse=tf.AUTO_REUSE):\n",
    "    fc_result = FC_layer(fc_img_drop)\n",
    "    #tf.get_variable_scope().reuse_variables()\n",
    "    conv_result = FC_layer(conv_img_drop)\n",
    "\n",
    "fc_pred = tf.nn.softmax(fc_result)\n",
    "conv_pred = tf.nn.softmax(conv_result)\n",
    "\n",
    "fc_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=fc_result, labels=ys))\n",
    "conv_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=conv_result, labels=ys))\n",
    "loss = fc_loss+conv_loss\n",
    "print(loss)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(Lr).minimize(loss)\n",
    "\n",
    "config = tf.ConfigProto() \n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if not os.path.exists('fc+conv_lstm_tmp_attention'):\n",
    "    os.mkdir('fc+conv_lstm_tmp_attention/')\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "init_op = tf.global_variables_initializer() \n",
    "sess.run(init_op)\n",
    "\n",
    "batch_fc_img = np.zeros((10*batch_size,40,4096),dtype = np.float32)\n",
    "batch_conv_img = np.zeros((10*batch_size,40,7,7,512),dtype = np.float32)\n",
    "batch_labels = np.zeros((10*batch_size,n_classes),dtype = np.int8)\n",
    "lr=0.01\n",
    "loss_,_ = sess.run([loss,train_op], feed_dict={\n",
    "            fc_img : batch_fc_img,\n",
    "            conv_img : batch_conv_img,\n",
    "            ys : batch_labels,\n",
    "            Lr : lr\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
